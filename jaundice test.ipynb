{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IFZFe2ae8sU"
      },
      "source": [
        "## Structure and populate the subfolders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZwpIFNYfDMf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "from torch import normal\n",
        "\n",
        "data_dir = \"./\"\n",
        "\n",
        "#create training dir\n",
        "training_dir = os.path.join(data_dir,\"training\")\n",
        "if not os.path.isdir(training_dir):\n",
        "  os.mkdir(training_dir)\n",
        "\n",
        "#create dog in training\n",
        "jaundice_training_dir = os.path.join(training_dir,\"jaundice\")\n",
        "if not os.path.isdir(jaundice_training_dir):\n",
        "  os.mkdir(jaundice_training_dir)\n",
        "\n",
        "#create cat in training\n",
        "normal_training_dir = os.path.join(training_dir,\"normal\")\n",
        "if not os.path.isdir(normal_training_dir):\n",
        "  os.mkdir(normal_training_dir)\n",
        "\n",
        "#create validation dir\n",
        "validation_dir = os.path.join(data_dir,\"validation\")\n",
        "if not os.path.isdir(validation_dir):\n",
        "  os.mkdir(validation_dir)\n",
        "\n",
        "#create dog in validation\n",
        "jaundice_validation_dir = os.path.join(validation_dir,\"jaundice\")\n",
        "if not os.path.isdir(jaundice_validation_dir):\n",
        "  os.mkdir(jaundice_validation_dir)\n",
        "\n",
        "#create cat in validation\n",
        "normal_validation_dir = os.path.join(validation_dir,\"normal\")\n",
        "if not os.path.isdir(normal_validation_dir):\n",
        "  os.mkdir(normal_validation_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cl6o5wvAfEcU"
      },
      "source": [
        "### Shuffle newly aquired data in folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUzGYjDpfKUs"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "from numpy import size\n",
        "\n",
        "split_size = 0.80\n",
        "jaundice_imgs_size = len(glob.glob(\"./jaundice/jaundice/*\"))\n",
        "normal_imgs_size = len(glob.glob(\"./normal/normal/*\"))\n",
        "\n",
        "\n",
        "for i,img in enumerate(glob.glob(\"./jaundice/jaundice/*\")):\n",
        "  if i < (jaundice_imgs_size * split_size):\n",
        "    shutil.move(img,jaundice_training_dir)\n",
        "\n",
        "  else:\n",
        "    shutil.move(img,jaundice_validation_dir)\n",
        "\n",
        "\n",
        "for i,img in enumerate(glob.glob(\"./normal/normal/*\")):\n",
        "  if i < (normal_imgs_size * split_size):\n",
        "    shutil.move(img,normal_training_dir)\n",
        "  else:\n",
        "    shutil.move(img,normal_validation_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRlVfednfT8J"
      },
      "source": [
        "## plot some examples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxj2kHRLfWYr"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from IPython.core.pylabtools import figsize\n",
        "\n",
        "samples_jaundice = [os.path.join(jaundice_training_dir,np.random.choice(a = os.listdir(jaundice_training_dir),size = 1)[0]) for _ in range(8)]\n",
        "samples_normal = [os.path.join(normal_training_dir,np.random.choice(a = os.listdir(normal_training_dir),size = 1)[0]) for _ in range(8)]\n",
        "\n",
        "nrows = 4\n",
        "ncols = 4\n",
        "\n",
        "fig, ax = plt.subplots(nrows,ncols,figsize = (10,10))\n",
        "ax = ax.flatten()\n",
        "\n",
        "for i in range(nrows*ncols):\n",
        "  if i < 8:\n",
        "    pic = plt.imread(samples_jaundice[i%8])\n",
        "    ax[i].imshow(pic)\n",
        "    ax[i].set_axis_off()\n",
        "  else:\n",
        "    pic = plt.imread(samples_normal[i%8])\n",
        "    ax[i].imshow(pic)\n",
        "    ax[i].set_axis_off()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Skin Detection code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# our custom transformer to detect only skin\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def custom_skin_detector(img):\n",
        "    img = np.array(img)\n",
        "    img_HSV = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "#skin color range for h in hsv\n",
        "    HSV_mask = cv2.inRange(img_HSV[:,:,0], np.array((0)), np.array((17)))\n",
        "    HSV_mask = cv2.morphologyEx(HSV_mask,cv2.MORPH_OPEN,np.ones((3,3),np.uint8))\n",
        "\n",
        "    #converting from gbr to YCbCr color space\n",
        "    img_YCrCb = cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)\n",
        "\n",
        "    #putting all values of y to 0\n",
        "    img_YCrCb[:,:,0] = 0\n",
        "\n",
        "    #skin color range for ycrcb color space \n",
        "    YCrCb_mask = cv2.inRange(img_YCrCb, np.array((0, 135, 85)), np.array((255,180,135))) \n",
        "    YCrCb_mask = cv2.morphologyEx(YCrCb_mask, cv2.MORPH_OPEN, np.ones((3,3), np.uint8))\n",
        "\n",
        "    #global mask made from YCrCb mask and hsv mask\n",
        "    global_mask=cv2.bitwise_and(YCrCb_mask,HSV_mask)\n",
        "    global_mask=cv2.medianBlur(global_mask,3)\n",
        "    global_mask = cv2.morphologyEx(global_mask, cv2.MORPH_OPEN, np.ones((4,4), np.uint8))\n",
        "    global_img = cv2.bitwise_and(img,img,mask=global_mask)\n",
        "    global_img = cv2.cvtColor(global_img,cv2.COLOR_BGR2RGB)\n",
        "    return global_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O--EXEvYfhOp"
      },
      "source": [
        "## Create dataloader\n",
        "\n",
        "Now we are going to do 3 things:\n",
        "\n",
        "1. Let’s preprocess our data using the compose method, which is a simple method to apply multiple preprocessing functions like normalization and data augmentation to our dataset.\n",
        "2. Let’s use ImageFolder to create a pytorch dataset. PyTorch infers the class automatically if the subdirectories structure is well defined (as in our case).\n",
        "3. Use the DataLoader to slice our data in batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import os\n",
        "# import cv2\n",
        "# import torch\n",
        "# from torch.utils.data import Dataset,DataLoader\n",
        "# from torchvision import transforms\n",
        "\n",
        "# # Custom dataset class to apply skin detection to images\n",
        "# class SkinDetectionDataset(Dataset):\n",
        "#     def __init__(self, folder_path, transform=None):\n",
        "#         self.folder_path = folder_path\n",
        "#         self.data = []  # A list to store (image, class_label) pairs\n",
        "#         for class_folder in os.listdir(folder_path):\n",
        "#             class_folder_path = os.path.join(folder_path, class_folder)\n",
        "#             if os.path.isdir(class_folder_path):\n",
        "#                 class_label = class_folder\n",
        "#                 for filename in os.listdir(class_folder_path):\n",
        "#                     if filename.endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
        "#                         image_path = os.path.join(class_folder_path, filename)\n",
        "#                         self.data.append((image_path, class_label))\n",
        "#         self.transform = transform\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         image_path, class_label = self.data[idx]\n",
        "#         image = cv2.imread(image_path)  # Load the image\n",
        "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
        "\n",
        "#         # Apply the skin detection function\n",
        "#         skin_detected_image = custom_skin_detector(image)\n",
        "\n",
        "#         if self.transform:\n",
        "#             skin_detected_image = self.transform(skin_detected_image)\n",
        "\n",
        "#         return skin_detected_image, class_label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# traindir = \"./training\"\n",
        "# testdir = \"./validation\"\n",
        "\n",
        "# # derfine a transformation to convert the image to pytorch tensor and resize it to 224\n",
        "# transform = transforms.Compose([transforms.ToTensor(),transforms.Resize((224,224))])\n",
        "\n",
        "# # Create an instance of the custom dataset\n",
        "# train_data = SkinDetectionDataset(traindir, transform= transform)\n",
        "# test_data = SkinDetectionDataset(testdir,transform = transform)\n",
        "\n",
        "# # Create a DataLoader for the dataset\n",
        "# batch_size = 16\n",
        "# trainloader = DataLoader(train_data,batch_size=batch_size,shuffle = True)\n",
        "# testloader = DataLoader(test_data,batch_size=batch_size,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Define your data loaders\n",
        "batch_size = 32\n",
        "num_workers = 4\n",
        "\n",
        "train_data = ImageFolder('./training', transform=ToTensor())\n",
        "test_data = ImageFolder('./validation', transform=ToTensor())\n",
        "\n",
        "trainloader = DataLoader(\n",
        "    dataset=train_data,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "testloader = DataLoader(\n",
        "    dataset=test_data,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "# Use your skin detection code with PyTorch's DataLoader\n",
        "for i, (images, labels) in enumerate(trainloader):\n",
        "    # Apply skin detection to each image in the batch\n",
        "    for j in range(images.shape[0]):\n",
        "        img = images[j].permute(1, 2, 0).numpy()\n",
        "        img = custom_skin_detector(img)\n",
        "        img = torch.from_numpy(img).permute(2, 0, 1)\n",
        "        images[j] = img\n",
        "\n",
        "    # Do something with the images and labels\n",
        "\n",
        "for i, (images, labels) in enumerate(testloader):\n",
        "    # Apply skin detection to each image in the batch\n",
        "    for j in range(images.shape[0]):\n",
        "        img = images[j].permute(1, 2, 0).numpy()\n",
        "        img = custom_skin_detector(img)\n",
        "        img = torch.from_numpy(img).permute(2, 0, 1)\n",
        "        images[j] = img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import cv2\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# import torchvision.transforms as transforms\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# # Load the image\n",
        "# image_path = './training/normal/normal (1).jpg'\n",
        "# image = cv2.imread(image_path)\n",
        "# image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# # Convert the image to YCbCr\n",
        "# yCbCr_image = cv2.cvtColor(image, cv2.COLOR_RGB2YCrCb)\n",
        "\n",
        "# # Extract the CbCr channels\n",
        "# CbCr_channels = yCbCr_image[:, :, 1:]\n",
        "\n",
        "# # Convert the image to HSV\n",
        "# hsv_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
        "\n",
        "# # Extract the Hue channel\n",
        "# hue_channel = hsv_image[:, :, 0]\n",
        "\n",
        "# # You can use a pre-trained face detection model here, for example, using OpenCV's Haar Cascade:\n",
        "# face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "# faces = face_cascade.detectMultiScale(image, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "\n",
        "# # Define a function to apply face detection based on CbCr channels\n",
        "# def detect_faces_CbCr(CbCr_image):\n",
        "#     # Define a threshold for detecting faces based on the CbCr values\n",
        "#     lower_bound = np.array([120, 133])\n",
        "#     upper_bound = np.array([180, 173])\n",
        "\n",
        "#     # Create a mask to select regions that fall within the CbCr range\n",
        "#     mask = cv2.inRange(CbCr_image, lower_bound, upper_bound)\n",
        "\n",
        "#     # Find contours in the mask\n",
        "#     contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "#     # Initialize an empty list to store detected face rectangles\n",
        "#     detected_faces = []\n",
        "\n",
        "#     for contour in contours:\n",
        "#         # Get the bounding box for each contour\n",
        "#         x, y, w, h = cv2.boundingRect(contour)\n",
        "\n",
        "#         # Add the bounding box to the list of detected faces\n",
        "#         detected_faces.append((x, y, x + w, y + h))\n",
        "\n",
        "#     return detected_faces\n",
        "\n",
        "# # Detect faces based on CbCr channels\n",
        "# CbCr_faces = detect_faces_CbCr(CbCr_channels)\n",
        "\n",
        "# # Display the image with CbCr face detection\n",
        "# image_copy = image.copy()\n",
        "# for (x, y, x2, y2) in CbCr_faces:\n",
        "#     cv2.rectangle(image_copy, (x, y), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "# # Display the image with CbCr face detection\n",
        "# cv2.imshow('CbCr Face Detection', image_copy)\n",
        "# cv2.waitKey(0)\n",
        "# cv2.destroyAllWindows()\n",
        "\n",
        "# # You can also use the hue channel for face detection or other purposes\n",
        "# # Detecting faces based on the hue channel is more challenging and may require additional techniques or models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47sto84vf6Pz"
      },
      "source": [
        "## training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vp6FicgZf5ni"
      },
      "outputs": [],
      "source": [
        "def make_train_step(model, optimizer, loss_fn):\n",
        "  def train_step(x,y):\n",
        "    #make prediction\n",
        "    yhat = model(x) #forward pass\n",
        "    #enter train mode\n",
        "    model.train() # make model to train\n",
        "    #compute loss\n",
        "    loss = loss_fn(yhat,y) # calculate loss\n",
        "\n",
        "    loss.backward() # backward propogation\n",
        "    optimizer.step() #optimizer step\n",
        "    optimizer.zero_grad() \n",
        "    #optimizer.cleargrads()\n",
        "\n",
        "    return loss\n",
        "  return train_step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZW4QHmMgEMX"
      },
      "source": [
        "## Our model used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVKIfA_0gGRC"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, models, transforms\n",
        "import torch.nn as nn\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = models.resnet34(pretrained=True)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "#add a new final layer\n",
        "with torch.no_grad():\n",
        "  nr_filters = model.fc.in_features  #number of input features of last layer\n",
        "  model.fc = nn.Linear(nr_filters, 1)\n",
        "\n",
        "  model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjDdumzqgLDy"
      },
      "source": [
        "### Optimizer and make train step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57Xx7rEWgPT2"
      },
      "outputs": [],
      "source": [
        "from torch.nn.modules.loss import BCEWithLogitsLoss\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "#loss\n",
        "loss_fn = BCEWithLogitsLoss() #binary cross entropy with sigmoid, so no need to use sigmoid in the model\n",
        "\n",
        "#optimizer\n",
        "optimizer = torch.optim.Adam(model.fc.parameters())\n",
        "\n",
        "#train step\n",
        "train_step = make_train_step(model, optimizer, loss_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3Hr3ZVugQwL"
      },
      "source": [
        "## Train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def accuracy_fn(y_true, y_pred):\n",
        "  correct = torch.sum((y_true == y_pred))\n",
        "  acc = (correct/len(y_pred))*100\n",
        "  return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "aZo6rJaIgUBV"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# %pip install tqdm\n",
        "from tqdm import tqdm\n",
        "\n",
        "losses = []\n",
        "val_losses = []\n",
        "\n",
        "epoch_train_losses = []\n",
        "epoch_test_losses = []\n",
        "\n",
        "n_epochs = 1\n",
        "early_stopping_tolerance = 3\n",
        "early_stopping_threshold = 0.03\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  print(f\"epoch {epoch} is running\")\n",
        "  epoch_loss = 0\n",
        "  for i ,data in tqdm(enumerate(trainloader), total = len(trainloader)): #iterate ovre batches\n",
        "    x_batch , y_batch = data\n",
        "    x_batch = x_batch.to(device) #move to gpu\n",
        "    y_batch = y_batch.unsqueeze(1).float() #convert target to same nn output shape\n",
        "    y_batch = y_batch.to(device)\n",
        "\n",
        "    loss = train_step(x_batch, y_batch)\n",
        "    epoch_loss += loss/len(trainloader)\n",
        "    losses.append(loss)\n",
        "\n",
        "  epoch_train_losses.append(epoch_loss)\n",
        "  print('\\nEpoch : {}, train loss : {}'.format(epoch+1,epoch_loss))\n",
        "\n",
        "  #validation doesnt requires gradient\n",
        "  with torch.no_grad():\n",
        "    cum_loss = 0\n",
        "    for data in testloader:\n",
        "      x_batch,y_batch = data\n",
        "      x_batch = x_batch.to(device)\n",
        "      y_batch = y_batch.unsqueeze(1).float() #convert target to same nn output shape\n",
        "      y_batch = y_batch.to(device)\n",
        "\n",
        "      #model to eval mode\n",
        "      model.eval()\n",
        "\n",
        "      yhat = model(x_batch)\n",
        "      val_loss = loss_fn(yhat,y_batch)\n",
        "      print(f\"accuracy: {accuracy_fn(yhat,y_batch)}\")\n",
        "      cum_loss += loss/len(testloader)\n",
        "      val_losses.append(val_loss.item())\n",
        "\n",
        "\n",
        "    epoch_test_losses.append(cum_loss)\n",
        "    print('Epoch : {}, val loss : {}'.format(epoch+1,cum_loss))\n",
        "\n",
        "    best_loss = min(epoch_test_losses)\n",
        "\n",
        "    #save best model\n",
        "    if cum_loss <= best_loss:\n",
        "      best_model_wts = model.state_dict()\n",
        "\n",
        "    #early stopping\n",
        "    early_stopping_counter = 0\n",
        "    if cum_loss > best_loss:\n",
        "      early_stopping_counter +=1\n",
        "\n",
        "    if (early_stopping_counter == early_stopping_tolerance) or (best_loss <= early_stopping_threshold):\n",
        "      print(\"/nTerminating: early stopping\")\n",
        "      break #terminate training\n",
        "\n",
        "#load best model\n",
        "model.load_state_dict(best_model_wts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cclwNASbgZRR"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6Ch-f2WgdHC"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def inference(test_data):\n",
        "  idx = torch.randint(1, len(test_data), (1,))\n",
        "  sample = torch.unsqueeze(test_data[idx][0], dim=0).to(device)\n",
        "\n",
        "  if torch.sigmoid(model(sample)) < 0.5:\n",
        "    print(\"Prediction : normal\")\n",
        "  else:\n",
        "    print(\"Prediction : jaundice\")\n",
        "\n",
        "\n",
        "  plt.imshow(test_data[idx][0].permute(1, 2, 0))\n",
        "\n",
        "inference(test_data)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
